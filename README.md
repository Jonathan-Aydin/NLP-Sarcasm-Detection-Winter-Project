{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-BoldOblique;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Bold;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}}
\margl1440\margr1440\vieww16640\viewh13780\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\i\b\fs48 \cf0 Sarcasm Detection in News Headlines Using BERT\

\f1\i0\b0\fs24 \
Name: Jonathan Aydin\
Email: gaboroaydin@gmail.com (jaydin@usc.edu)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \

\f2\b\fs36 Project Overview\

\f1\b0 \

\fs24 In this project, I developed a Natural Language Processing (NLP) model to detect sarcasm in news headlines. The model, leveraging a pre-trained BERT architecture, aims to discern sarcastic tones in textual data, a challenging aspect of language understanding.\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\

\f2\b\fs36 Dataset
\f1\b0 \

\fs24 \

\fs28 \ul Description and Preprocessing\
\

\fs24 \ulnone We used the "News Headlines Dataset for Sarcasm Detection", sourced from TheOnion and HuffPost. The dataset contains headlines labeled as sarcastic (is_sarcastic: 1) or not (is_sarcastic: 0). To prepare the data for our model:\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	\uc0\u8226 	}
\f2\b Tokenization
\f1\b0 : Utilizing the BertTokenizer from the Hugging Face transformers library, headlines were broken down into tokens. This process converts text into a format that is compatible with the BERT model. The choice of tokenizer directly aligns with the pre-trained model's understanding of language structure.\
{\listtext	\uc0\u8226 	}
\f2\b Sequence Length Standardization
\f1\b0 : Given the variability in headline lengths, each input sequence was standardized to a length of 128 tokens. This length was chosen to ensure enough contextual information while maintaining computational efficiency. Sequences shorter than 128 tokens were padded, and longer ones were truncated.\
{\listtext	\uc0\u8226 	}
\f2\b Train-Test Split
\f1\b0 : The dataset was divided into a training set (85%) and a testing set (15%). This split ensured a substantial amount of data for model training while reserving an adequate portion for model evaluation.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
The dataset's composition of professionally written headlines means fewer anomalies like slang or misspellings, which aligns well with BERT's training on formal text sources. Standardizing sequence length and data splitting was crucial to prepare the data for effective model training and unbiased evaluation.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \

\f2\b\fs36 Model Development and Training\
\

\fs28 Implementation Choices\

\f1\b0\fs24 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	\uc0\u8226 	}
\f2\b Model
\f1\b0 : BERT (bert-base-uncased), known for its deep understanding of language nuances and context, was an obvious choice for sarcasm detection, which is inherently subtle and context-dependent.\
{\listtext	\uc0\u8226 	}
\f2\b Architecture
\f1\b0 : A dense layer with a sigmoid activation function was added on top of the BERT model for binary classification. This layer interprets the representations from BERT and outputs the probability of the headline being sarcastic.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b\fs28 \cf0 Training Procedure
\f1\b0\fs24 \

\f2\b\fs28 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls3\ilvl0
\f1\b0\fs24 \cf0 {\listtext	\uc0\u8226 	}
\f2\b Optimizer
\f1\b0 : Adam, selected for its effectiveness in handling sparse gradients and its adaptive learning rate capabilities, which are beneficial for fine-tuning tasks.\
{\listtext	\uc0\u8226 	}
\f2\b Learning Rate
\f1\b0 : Set to 1e-5, a lower rate to make small, incremental adjustments to the pre-trained model, reducing the risk of overfitting.\
{\listtext	\uc0\u8226 	}
\f2\b Loss Function
\f1\b0 : Binary Cross-Entropy, ideal for binary classification tasks as it quantifies the difference between the predicted probabilities and the actual labels.\
{\listtext	\uc0\u8226 	}
\f2\b Epochs
\f1\b0 : Limited to 3 to balance between sufficient learning and the risk of overfitting, considering BERT's sensitivity to over-training.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b\fs28 \cf0 Hyperparameters\

\f1\b0\fs24 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls4\ilvl0\cf0 {\listtext	\uc0\u8226 	}
\f2\b Batch Size
\f1\b0 : 32, a recommended size for fine-tuning BERT.\
{\listtext	\uc0\u8226 	}
\f2\b Maximum Sequence Length
\f1\b0 : Set to 128 tokens to capture sufficient context from each headline.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b\fs36 \cf0 Model Evaluation/Results\

\fs28 \

\fs26 Performance Metrics
\fs24 \

\f1\b0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	\uc0\u8226 	}
\f2\b Accuracy
\f1\b0 : Approximately 96%. This high accuracy indicates that the model can correctly identify sarcasm in most cases.\
{\listtext	\uc0\u8226 	}
\f2\b Loss
\f1\b0 : Approximately 0.0942. Observed both in training and validation. The training loss continued to decrease, while the validation loss showed slight increases, suggesting overfitting.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b\fs36 \cf0 Results
\f1\b0\fs24 \
The model achieved an accuracy of around 96.46% with a loss of 0.0942. However, there was a noticeable divergence between training and validation loss, suggesting possible overfitting.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\

\f2\b\fs36 Discussion\

\f1\b0\fs24 \

\f2\b\fs28 Fit for Task\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls6\ilvl0
\f1\b0\fs24 \cf0 {\listtext	\uc0\u8226 	}Dataset: The dataset's professional writing style aligns well with BERT's pre-training on diverse and formal language sources.\
{\listtext	\uc0\u8226 	}Model Architecture: BERT's deep understanding of language context makes it a strong fit for detecting sarcasm, which often relies on subtle language cues.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b\fs28 \cf0 Wider Implications and Social Good\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls7\ilvl0
\f1\b0\fs24 \cf0 {\listtext	\uc0\u8226 	}Content Moderation: This model can aid in flagging sarcastic content, which is crucial in contexts like news dissemination and social media.\
{\listtext	\uc0\u8226 	}Sentiment Analysis: An extension of this work could contribute to more nuanced sentiment analysis tools.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \

\f2\b\fs28 Limitations
\f1\b0\fs24 \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	\uc0\u8226 	}Context Dependence: Sarcasm often relies on external context, which may not be fully captured by the model.\
{\listtext	\uc0\u8226 	}Data Diversity: The dataset, though high quality, is limited to two sources, which might affect the model's generalizability.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \

\fs28 Next Steps\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0
\fs24 \cf0 {\listtext	\uc0\u8226 	}Incorporate Context: Experiment with models that can factor in external context.\
{\listtext	\uc0\u8226 	}Dataset Expansion: Include more diverse data sources for training.\
{\listtext	\uc0\u8226 	}Hyperparameter Tuning: Further tuning and regularization to address overfitting.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 In conclusion, this project demonstrates the feasibility of using advanced NLP models like BERT for nuanced language tasks such as sarcasm detection. While promising, the model's current limitations highlight the need for more context-aware and diverse training approaches in the field of sentiment analysis.}